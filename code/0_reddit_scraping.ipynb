{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_post_scraping(url, post_count):\n",
    "    posts = []\n",
    "    after = None\n",
    "    pages = round(post_count/25)\n",
    "\n",
    "    for a in range(pages):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "        \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,60)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_post_scraping_flair_filter(url, post_count, flair_name):\n",
    "    posts = []\n",
    "    after = None\n",
    "    pages = round(post_count/25)\n",
    "\n",
    "    for a in range(pages):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '&after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "        \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        \n",
    "        # Add tag to posts\n",
    "        for post in current_posts:\n",
    "            post['tag'] = flair_name\n",
    "\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,15)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through the posts, 25 posts at a time (total of ~1000 posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape main subreddit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steam Deck subreddit\n",
    "steam_deck_posts = subreddit_post_scraping(url='https://www.reddit.com/r/SteamDeck.json', post_count=5000)\n",
    "\n",
    "# Save to csv\n",
    "pd.DataFrame(steam_deck_posts).to_csv('../data/steam_deck_reddit_posts.csv', index=False)\n",
    "pd.DataFrame(steam_deck_posts).to_json('../data/steam_deck_reddit_posts.json', orient='records', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape flair filtering page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_url_list = {\n",
    "    'Picture': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Picture%22',\n",
    "    'Configuration': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Configuration%22',\n",
    "    'Video': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Video%22',\n",
    "    'Guide': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Guide%22',\n",
    "    'Meme / Shitpost': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Meme%20%2F%20Shitpost%22',\n",
    "    'News': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22News%22',\n",
    "    'MEGATHREAD': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22MEGATHREAD%22',\n",
    "    'Feature Request': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Feature%20Request%22',\n",
    "    'Hot Wasabi': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Hot%20Wasabi%22',\n",
    "    'Meta': 'https://www.reddit.com/r/SteamDeck/search.json?q=flair%3A%22Meta%22'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steam Deck subreddit\n",
    "steam_deck_posts_flair = []\n",
    "for flair in flair_url_list:\n",
    "    steam_deck_posts_flair += subreddit_post_scraping_flair_filter(url=flair_url_list[flair], post_count=500, flair_name=flair)\n",
    "\n",
    "# Save to csv\n",
    "pd.DataFrame(steam_deck_posts_flair).to_csv('../data/steam_deck_reddit_posts_flair.csv', index=False)\n",
    "pd.DataFrame(steam_deck_posts_flair).to_json('../data/steam_deck_reddit_posts_flair.json', orient='records', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
